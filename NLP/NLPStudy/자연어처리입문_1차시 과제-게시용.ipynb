{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4dabd919",
   "metadata": {},
   "source": [
    "# 1. 자연어처리 토큰화 라이브러리 활용\n",
    "---\n",
    "* 자연어처리 분야에서 전처리 과정에 활용할 수 있는 다양한 라이브러리가 존재합니다.\n",
    "* 필요에 따라 활용할 수 있는 활용도를 연습해봅니다.\n",
    "> `don't`와 `Mr.`, `Jone's` 등의 단어 토큰화 결과를 보고 사용한 라이브러리를 예측해주세요.  \n",
    "`word_tokenize`, `WordPunctTokenizer`, `text_to_word_sequence` 중 골라주세요.  \n",
    ">`*None*`을 채워주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c73b58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 17:57:07.819182: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화를 거친 결과1 : ['particularly', 'on', 'prisoner', 'swaps', 'we’ve', 'listened', 'to', 'ukraine', 'we', 'are', 'also', 'listening', 'to', 'russia', 'i', 'spoke', 'to', 'mr', 'putin']\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 import\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "print('단어 토큰화를 거친 결과1 :',text_to_word_sequence(\"Particularly on prisoner swaps, we’ve listened to Ukraine. We are also listening to Russia. I spoke to Mr. Putin.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42910286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화를 거친 결과2 : ['It', '’', 's', 'a', 'real', 'problem', 'for', 'society', 'when', 'a', 'few', 'dozen', 'people', 'and', 'companies', 'own', 'every', 'single', 'thing', 'so', 'that', 'no', 'alternative', 'paradigms', 'can', 'exist', 'that', 'they', 'don', '’', 't', 'co-opt', 'from', 'the', 'cradle', '.']\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 import\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print('단어 토큰화를 거친 결과2 :', word_tokenize(\"It’s a real problem for society when a few dozen people and companies own every single thing so that no alternative paradigms can exist that they don’t co-opt from the cradle.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a306a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화를 거친 결과3 : ['Mr', '.', 'Mikleborough', '[', 'sic', ']', 'regularly', 'texted', 'me', ',', 'and', 'many', 'of', 'the', 'messages', 'were', 'informal', '.']\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 import\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "\n",
    "print(\"단어 토큰화를 거친 결과3 :\", WordPunctTokenizer().tokenize(\"Mr. Mikleborough [sic] regularly texted me, and many of the messages were informal.\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0612e4e3",
   "metadata": {},
   "source": [
    "# 2. 표준 토큰화 예제\n",
    "---\n",
    "> `Penn Treebank Tokenization`의 규칙\n",
    "* 규칙 1. 하이푼으로 구성된 단어는 하나로 유지한다.\n",
    "* 규칙 2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb553eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표준 토큰화 : ['My', 'dream', 'car', 'is', 'Porsche-911.', 'But', 'I', 'do', \"n't\", 'know', 'if', 'I', 'can', 'buy']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# 규칙을 참고하여 문장을 작성해보고, 실행해주세요.\n",
    "text = \"My dream car is Porsche-911. But I don't know if I can buy\"\n",
    "print('표준 토큰화 :',tokenizer.tokenize(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "527edd23",
   "metadata": {},
   "source": [
    "# 3. 한국어에서 불용어(stopwords) 제거하기\n",
    "---\n",
    "> 생각해볼 것\n",
    "> * 불용어란 무엇인가요?\n",
    "> * 자연어처리를 할 때 왜 불용어를 제거해야하나요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "407c9441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 : ['블로그', '챌', '린지', '참여', '를', '하고자', '오늘', '은', '주간', '일기', '를', '씁니다', 'ㅎ', '잇님들', '포스팅', '중', '에', '주간', '일기', '나', '월간', '일기', '작성', '하시는', '분들', '보면', '우아', '맛있는거', '많이', '드시고', '이렇게', '지내고', '계셨구나', '~', '하면서', '재미있게', '봤던', '기억', '이', '있는데', '이번', '엔', '저', '도', '참여', '를', '합니다', '+_+']\n",
      "불용어 제거 후 : ['블로그', '오늘', '주간', '일기', '씁니다', '포스팅', '중', '에', '주간', '일기', '월간', '일기', '작성', '보면', '맛있는거', '많이', '재미있게', '기억', '이번']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "example = \"\"\"블로그 챌린지 참여를 하고자 오늘은 주간일기를 씁니다 ㅎ 잇님들 포스팅 중 에 주간일기나 월간일기 작성하시는 분들 보면 우아 맛있는거 많이 드시고 이렇게 지내고 계셨구나~ \n",
    "하면서 재미있게 봤던 기억이 있는데 이번엔 저도 참여를 합니다 +_+\"\"\"\n",
    "\n",
    "stop_words = \"챌 린지 참여 를 하고자 은 를 ㅎ 잇님들 나 하시는 분들 우아 드시고 이렇게 지내고 계셨구나 ~ 이 하면서 봤던 있는데 엔 저 도 참여 를 합니다 +_+\"\n",
    "\n",
    "stop_words = set(stop_words.split(' '))\n",
    "word_tokens = okt.morphs(example)\n",
    "\n",
    "result = [word for word in word_tokens if not word in stop_words]\n",
    "\n",
    "print('불용어 제거 전 :', word_tokens) \n",
    "print('불용어 제거 후 :', result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b2e1a1e",
   "metadata": {},
   "source": [
    "# 4. Keras 전처리 도구로 패딩(Padding)하기\n",
    "---\n",
    "> * 자연어처리에서 padding 처리가 왜 필요한가요?\n",
    "> * 기억나지 않는다면 다시 공부하세요.\n",
    "> * Numpy로도 가능하지만, Keras에서 제공하는 전처리 도구로 실습해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d346d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 6, 1], [7, 9, 2], [10, 2], [2, 3, 1, 2], [5, 4, 1], [1, 11, 3], [1, 2, 6], [1, 8, 12], [7, 1, 2, 13, 3, 8, 14, 4], [1, 15, 4, 1, 16]]\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 Import\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 단어 집합을 만든다.\n",
    "preprocessed_sentences = [['고기', '맛있다'], ['고기', '좋은', '고기'], \n",
    "                          ['돼지', '오늘', '맛집'], ['밑반찬', '맛집'], \n",
    "                          ['맛집', '행복', '고기', '맛집'], ['맛있다', '소', '고기'], \n",
    "                          ['고기', '시간', '행복'], ['고기', '맛집', '좋은'], \n",
    "                          ['고기', '육즙', '좋다'], ['돼지', '고기', '맛집', '영업비밀', '행복', '육즙', '꿀맛','소'], \n",
    "                          ['고기', '지난주', '소','고기', '다녀왔다']]\n",
    "\n",
    "# 정수인코딩\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d8d0054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  1  5]\n",
      " [ 0  0  0  0  0  1  6  1]\n",
      " [ 0  0  0  0  0  7  9  2]\n",
      " [ 0  0  0  0  0  0 10  2]\n",
      " [ 0  0  0  0  2  3  1  2]\n",
      " [ 0  0  0  0  0  5  4  1]\n",
      " [ 0  0  0  0  0  1 11  3]\n",
      " [ 0  0  0  0  0  1  2  6]\n",
      " [ 0  0  0  0  0  1  8 12]\n",
      " [ 7  1  2 13  3  8 14  4]\n",
      " [ 0  0  0  1 15  4  1 16]]\n"
     ]
    }
   ],
   "source": [
    "# 패딩처리\n",
    "padded = pad_sequences(encoded)\n",
    "print(padded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72706ece",
   "metadata": {},
   "source": [
    "* 왜 `column` 이 `8`인가요?\n",
    "> 가장 긴 문자열에 자동으로 맞추어 `0`으로 채우기 때문입니다.\n",
    "* 데이터 전처리 시간 단축을 위해 `8`이 아닌 `5`로 지정하여봅시다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d86832de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  1,  5],\n",
       "       [ 0,  0,  1,  6,  1],\n",
       "       [ 0,  0,  7,  9,  2],\n",
       "       [ 0,  0,  0, 10,  2],\n",
       "       [ 0,  2,  3,  1,  2],\n",
       "       [ 0,  0,  5,  4,  1],\n",
       "       [ 0,  0,  1, 11,  3],\n",
       "       [ 0,  0,  1,  2,  6],\n",
       "       [ 0,  0,  1,  8, 12],\n",
       "       [13,  3,  8, 14,  4],\n",
       "       [ 1, 15,  4,  1, 16]], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(encoded, maxlen=5)\n",
    "padded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7de0c48e",
   "metadata": {},
   "source": [
    "기본적으로 문서 뒤가 아니라, `앞`에서 0으로 채운다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ccd8b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  0,  0,  0],\n",
       "       [ 1,  6,  1,  0,  0],\n",
       "       [ 7,  9,  2,  0,  0],\n",
       "       [10,  2,  0,  0,  0],\n",
       "       [ 2,  3,  1,  2,  0],\n",
       "       [ 5,  4,  1,  0,  0],\n",
       "       [ 1, 11,  3,  0,  0],\n",
       "       [ 1,  2,  6,  0,  0],\n",
       "       [ 1,  8, 12,  0,  0],\n",
       "       [13,  3,  8, 14,  4],\n",
       "       [ 1, 15,  4,  1, 16]], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  뒤에서부터 0으로 채우도록 해보자.\n",
    "padded = pad_sequences(encoded, padding=\"post\", maxlen=5)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1aae177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  0,  0,  0],\n",
       "       [ 1,  6,  1,  0,  0],\n",
       "       [ 7,  9,  2,  0,  0],\n",
       "       [10,  2,  0,  0,  0],\n",
       "       [ 2,  3,  1,  2,  0],\n",
       "       [ 5,  4,  1,  0,  0],\n",
       "       [ 1, 11,  3,  0,  0],\n",
       "       [ 1,  2,  6,  0,  0],\n",
       "       [ 1,  8, 12,  0,  0],\n",
       "       [ 7,  1,  2, 13,  3],\n",
       "       [ 1, 15,  4,  1, 16]], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 손실이 걱정된다.\n",
    "# 앞이 아니라 뒷 단어를 삭제하고싶다면? \n",
    "padded = pad_sequences(encoded, padding='post', truncating='post', maxlen=5)\n",
    "padded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52b9b226",
   "metadata": {},
   "source": [
    "# 5. Keras를 이용한 원-핫 인코딩\n",
    "> 단어 집합(vocabulary)에 있는 단어들로만 구성된 텍스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd61b8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합 : {'영화': 1, '고릴라': 2, '나랑': 3, '보러': 4, '갈래': 5, '제목': 6, '볼래': 7, '재밌대': 8}\n"
     ]
    }
   ],
   "source": [
    "# keras tokenizer를 이용한 정수 인코딩\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "text = \"나랑 영화 보러 갈래 영화 제목 고릴라 볼래 영화 고릴라 재밌대\"\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "print(\"단어 집합 :\", tokenizer.word_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0732522e",
   "metadata": {},
   "source": [
    "`texts_to_sequences()`를 통해 정수 시퀀스로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "511f4c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 5, 6, 2, 8]\n"
     ]
    }
   ],
   "source": [
    "sub_text = \"영화 보러 갈래 제목 고릴라 재밌대\"\n",
    "encoded = tokenizer.texts_to_sequences([sub_text])[0]\n",
    "print(encoded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7467a541",
   "metadata": {},
   "source": [
    "원-핫 인코딩을 수행하는 `to_categorical()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51391f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot = to_categorical(encoded)\n",
    "one_hot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63ef22be",
   "metadata": {},
   "source": [
    "# 6. 한국어 전처리 (사전 등록)\n",
    "> 고유명사를 형태소 단위로 나누는 경우, 나누지 말라고 사전에 등록해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "997b1a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting customized_konlpy\n",
      "  Downloading customized_konlpy-0.0.64-py3-none-any.whl (881 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.5/881.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Jpype1>=0.6.1 in /Users/leewonseok/anaconda3/lib/python3.10/site-packages (from customized_konlpy) (1.4.1)\n",
      "Requirement already satisfied: konlpy>=0.4.4 in /Users/leewonseok/anaconda3/lib/python3.10/site-packages (from customized_konlpy) (0.6.0)\n",
      "Requirement already satisfied: packaging in /Users/leewonseok/anaconda3/lib/python3.10/site-packages (from Jpype1>=0.6.1->customized_konlpy) (22.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /Users/leewonseok/anaconda3/lib/python3.10/site-packages (from konlpy>=0.4.4->customized_konlpy) (4.9.1)\n",
      "Requirement already satisfied: numpy>=1.6 in /Users/leewonseok/anaconda3/lib/python3.10/site-packages (from konlpy>=0.4.4->customized_konlpy) (1.23.5)\n",
      "Installing collected packages: customized_konlpy\n",
      "Successfully installed customized_konlpy-0.0.64\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install customized_konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "296f2521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leewonseok/anaconda3/lib/python3.10/site-packages/konlpy/tag/_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['민혁', '이', '가', '스타벅스', '에서', '자몽', '허니', '블랙', '티', '를', '마셨다', '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# customized_konlpy에서 제공하는 형태소 분석기 twitter를 사용하여 앞서 소개한 예문을 단어 토큰화\n",
    "from ckonlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "twitter.morphs('민혁이가 스타벅스에서 자몽허니블랙티를 마셨다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15918221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전에 추가\n",
    "twitter.add_dictionary(\"자몽허니블랙티\", 'Noun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34e1e1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['민혁', '이', '가', '스타벅스', '에서', '자몽허니블랙티', '를', '마셨다', '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사전에 추가한 뒤 결과 확인하기\n",
    "twitter.morphs('민혁이가 스타벅스에서 자몽허니블랙티를 마셨다.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28159bbf",
   "metadata": {},
   "source": [
    "# 7. 반복되는 문자 정제하기\n",
    "> 한국어의 경우 `ㅋㅋㅋㅋㅋㅋㅋ` `ㅠㅠㅠ` `ㅎㅎㅎㅎㅎ` 반복되는 불필요한 문자열이 많다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "672b7806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.normalizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af9a7c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "앜ㅋㅋ넘웃겨ㅠㅠ\n"
     ]
    }
   ],
   "source": [
    "# 반복되는 문자는 두번씩만 print 한다.\n",
    "text = \"앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ넘웃겨ㅠㅠㅠㅠㅠㅠㅠㅠ\"\n",
    "print(repeat_normalize(text, num_repeats = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2203728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "와하하핫\n"
     ]
    }
   ],
   "source": [
    "# 반복되는 문자는 두번씩만 print 한다.\n",
    "print(repeat_normalize('와하하하하하하하하하핫', num_repeats = 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
